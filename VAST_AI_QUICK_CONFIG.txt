â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
VAST.AI TEMPLATE - QUICK COPY-PASTE CONFIGURATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. VM IMAGE PATH                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
docker.io/vastai/kvm:@vastai-automatic-tag

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. PORTS (one per line)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
5000:5000
3000:3000
19530:19530
9091:9091

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. DOCKER OPTIONS                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
--shm-size=16g --gpus all -v /root:/workspace

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ENVIRONMENT VARIABLES (key=value, one per line)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
DATABASE_URL=sqlite:///./instance/llm_rag.db
MILVUS_HOST=localhost
MILVUS_PORT=19530
VLLM_BASE_URL=http://localhost:8000
TRITON_BASE_URL=http://localhost:8001
ADAPTER_BASE_PATH=/workspace/LLM_RAG/LLM-Finetuner-main/adapters
BYPASS_LOGIN=true
FLASK_APP=app_new.py
PORT=5000
CUDA_AVAILABLE=true
GUNICORN_WORKERS=4
REACT_APP_API_HOST=http://localhost:5000
REACT_APP_BYPASS_LOGIN=true
REACT_APP_GOOGLE_CLIENT_ID=
NODE_ENV=production

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. ON-START SCRIPT                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(LEAVE EMPTY - We'll deploy manually after instance starts)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. EXTRA FILTERS (optional, for GPU selection)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
gpu_name:RTX 5090
gpu_name:RTX 5070 Ti
gpu_name:RTX 4090
gpu_name:RTX 4080
gpu_name:A100

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. DISK SPACE                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Minimum: 50 GB
Recommended: 100 GB

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AFTER CREATING TEMPLATE - MANUAL DEPLOYMENT STEPS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ STEP 1: Create Template
   â€¢ Fill in all fields above (leave On-start Script EMPTY)
   â€¢ Click "Save Template"
   â€¢ Select a GPU instance and click "RENT"

ğŸ“‹ STEP 2: Connect to Instance
   â€¢ Wait for instance to start (1-2 minutes)
   â€¢ Copy SSH command from vast.ai dashboard
   â€¢ SSH into the instance:
     ssh -p <PORT> root@<IP>

ğŸ“‹ STEP 3: Run Deployment Script
   â€¢ Once connected, run this command:
   
   curl -fsSL https://raw.githubusercontent.com/dragon-dev-01/LLM_RAG/main/vast_ai_onstart.sh | bash
   
   OR manually copy-paste the deployment commands (see below)

ğŸ“‹ STEP 4: Wait for Deployment
   â€¢ First deployment takes 5-10 minutes
   â€¢ Watch for "âœ… Deployment Complete!" message
   â€¢ Services will be available at:
     â€¢ Backend API:  http://<INSTANCE_IP>:5000
     â€¢ Frontend UI:  http://<INSTANCE_IP>:3000

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MANUAL DEPLOYMENT COMMANDS (if script doesn't work)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Install dependencies
apt-get update -y
apt-get install -y python3 python3-pip python3-venv git curl wget build-essential docker.io docker-compose nodejs npm screen sqlite3 libpq-dev poppler-utils tesseract-ocr libtesseract-dev libgl1-mesa-glx libglib2.0-0
curl -fsSL https://deb.nodesource.com/setup_18.x | bash - && apt-get install -y nodejs

# Clone repository
cd /root && git clone https://github.com/dragon-dev-01/LLM_RAG.git LLM_RAG || (cd LLM_RAG && git pull)

# Start MilvusDB
cd /root/LLM_RAG/LLM-Finetuner-main && docker-compose up -d
sleep 30

# Setup Python backend
cd /root/LLM_RAG/LLM-Finetuner-main
python3 -m venv venv
/root/LLM_RAG/LLM-Finetuner-main/venv/bin/pip install --upgrade pip setuptools wheel
/root/LLM_RAG/LLM-Finetuner-main/venv/bin/pip install -r requirements.txt gunicorn

# Create directories
mkdir -p uploads adapters logs instance

# Initialize database
export DATABASE_URL=sqlite:///./instance/llm_rag.db
export MILVUS_HOST=localhost
export MILVUS_PORT=19530
export BYPASS_LOGIN=true
export FLASK_APP=app_new.py
export PORT=5000
cd /root/LLM_RAG/LLM-Finetuner-main
/root/LLM_RAG/LLM-Finetuner-main/venv/bin/python3 -m flask db upgrade || /root/LLM_RAG/LLM-Finetuner-main/venv/bin/python3 -c "from src import db, create_app; app = create_app(); app.app_context().push(); db.create_all()"
/root/LLM_RAG/LLM-Finetuner-main/venv/bin/python3 scripts/init_base_models.py || touch .base_models_initialized

# Setup frontend
cd /root/LLM_RAG/LLM-Finetuner-FE-main
npm install --legacy-peer-deps
echo "REACT_APP_API_HOST=http://localhost:5000" > .env
echo "REACT_APP_BYPASS_LOGIN=true" >> .env
echo "PORT=3000" >> .env

# Start backend
cd /root/LLM_RAG/LLM-Finetuner-main
screen -dmS backend bash -c "export DATABASE_URL=sqlite:///./instance/llm_rag.db && export MILVUS_HOST=localhost && export MILVUS_PORT=19530 && export BYPASS_LOGIN=true && export PORT=5000 && /root/LLM_RAG/LLM-Finetuner-main/venv/bin/gunicorn -w 4 -b 0.0.0.0:5000 --timeout 300 app_new:app"

# Start frontend
sleep 5
cd /root/LLM_RAG/LLM-Finetuner-FE-main
screen -dmS frontend bash -c "BROWSER=none npm start"

# Check status
sleep 10
echo "âœ… Deployment Complete!"
echo "Backend: http://localhost:5000"
echo "Frontend: http://localhost:3000"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
USEFUL COMMANDS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ View logs:
   screen -r backend   # Backend logs
   screen -r frontend # Frontend logs
   (Press Ctrl+A then D to detach)

ğŸ§ª Test API:
   curl http://localhost:5000/api/base-models

ğŸ”„ Restart services:
   screen -r backend   # Then Ctrl+C, restart command
   screen -r frontend   # Then Ctrl+C, restart command

ğŸ›‘ Stop services:
   pkill -f gunicorn
   pkill -f "npm start"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

